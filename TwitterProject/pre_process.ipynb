{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\raych\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import webcolors\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('wordnet')\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featureSelect(init_data):\n",
    "    #Feature Selection\n",
    "    #Filter out attributes such as unit_id, golden, judegements that aren't relevant for data mining.\n",
    "    drop_attributes = ['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
    "                       '_last_judgment_at','gender:confidence', 'profile_yn',\n",
    "                       'profile_yn:confidence', 'created','gender_gold','profile_yn_gold',\n",
    "                       'tweet_created', 'tweet_id']\n",
    "    init_data = init_data.drop(drop_attributes,axis=1)\n",
    "    #Remove attributes with too many miss values - tweet_coord\n",
    "    init_data = init_data.drop(['tweet_coord'],axis=1)\n",
    "    #Remove rows with missing gender, unknown gender , or is brand gender - lose around 1/3 of the data\n",
    "        #init_data.isnull().sum()\n",
    "    init_data = init_data.dropna(subset=['gender'])\n",
    "    init_data = init_data[init_data.gender != 'brand']\n",
    "    init_data = init_data[init_data.gender != 'unknown']\n",
    "\n",
    "    #drop retweet count because only 2.6% of male & female accounts combined have value > 0\n",
    "    init_data = init_data.drop(['retweet_count'],axis=1)\n",
    "\n",
    "    #drop profileimage for now & tweet/timezon locations for now\n",
    "    init_data = init_data.drop(['profileimage'],axis=1)\n",
    "    init_data = init_data.drop(['tweet_location', 'user_timezone'],axis=1)\n",
    "    return init_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def colorReplacement(init_data):\n",
    "    #replace each hexidecimal link and sidebar color with 3 new attributes being rgb percentages in decimal\n",
    "    side_bar = init_data['sidebar_color']\n",
    "    link_bar = init_data['link_color']\n",
    "\n",
    "    hexi_rgb = []\n",
    "    for hexi_side in side_bar:\n",
    "        rgb_val = ('0%','0%','0%')\n",
    "        if((hexi_side != \"0\") and (len(hexi_side) == 6)):\n",
    "            rgb_val = webcolors.hex_to_rgb_percent(\"#\" + hexi_side.lower())\n",
    "            hexi_rgb.append(rgb_val)\n",
    "        else:\n",
    "            hexi_rgb.append(rgb_val)\n",
    "\n",
    "    link_rgb = []\n",
    "    for hexi_link in link_bar:\n",
    "        rgb_val = ('0%','0%','0%')\n",
    "        if((hexi_link != \"0\") and (len(hexi_link) == 6)):\n",
    "            rgb_val = webcolors.hex_to_rgb_percent(\"#\" + hexi_link)\n",
    "            link_rgb.append(rgb_val)\n",
    "        else:\n",
    "            link_rgb.append(rgb_val)\n",
    "\n",
    "    init_data = init_data.reset_index(drop=True)\n",
    "    side_colors = pd.DataFrame(hexi_rgb,columns= ['sred','sgreen','sblue'])\n",
    "    link_colors = pd.DataFrame(link_rgb,columns = ['lred','lgreen','lblue'])\n",
    "    init_data = init_data.join(side_colors)\n",
    "    init_data = init_data.join(link_colors)\n",
    "    init_data['sred'] = init_data['sred'].str.rstrip('%').astype('float') / 100.0\n",
    "    init_data['sgreen'] = init_data['sgreen'].str.rstrip('%').astype('float') / 100.0\n",
    "    init_data['sblue'] = init_data['sblue'].str.rstrip('%').astype('float') / 100.0\n",
    "    init_data['lred'] = init_data['lred'].str.rstrip('%').astype('float') / 100.0\n",
    "    init_data['lgreen'] = init_data['lgreen'].str.rstrip('%').astype('float') / 100.0\n",
    "    init_data['lblue'] = init_data['lblue'].str.rstrip('%').astype('float') / 100.0\n",
    "    init_data = init_data.drop(['sidebar_color','link_color'],axis=1)\n",
    "    return init_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textProcess(init_data, checkifTestSet):\n",
    "    # Text pre-processing \n",
    "    global lemma_intersect \n",
    "    global Lemma_femaleWords \n",
    "    global Lemma_maleWords\n",
    "    global stem_intersect \n",
    "    global Stem_femaleWords \n",
    "    global Stem_maleWords\n",
    "    #all descriptions that have nan value replace with empty string, text has no null values.\n",
    "    init_data['description'].fillna('',inplace=True)\n",
    "\n",
    "    #clean text and description\n",
    "    description = init_data['description']\n",
    "    text = init_data['text']\n",
    "    gender = init_data['gender']\n",
    "    lemma = WordNetLemmatizer()\n",
    "    Pstem = PorterStemmer()\n",
    "    Lemma_descriptionList = []   # List of clean descriptions words for each record\n",
    "    Lemma_textList = []          # List of clean text words for each record \n",
    "    Lemma_femaleWords = set()\n",
    "    Lemma_maleWords = set()\n",
    "\n",
    "    Stem_descriptionList = []   \n",
    "    Stem_textList = []           \n",
    "    Stem_femaleWords = set()\n",
    "    Stem_maleWords = set()\n",
    "\n",
    "    # Remove symbols from stopword list - stopwords obtained from https://www.ranks.nl/stopwords\n",
    "    stopwords = open(\"longstopwordList.txt\").read().splitlines()\n",
    "    clean_stopwords = [re.sub(r'[^\\w]', '',word) for word in stopwords]\n",
    "    stopwordsSet = set(clean_stopwords)\n",
    "\n",
    "    #loop for clean each text attribute in each record\n",
    "    pattern = \"(@[A-Za-z]+)|([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^RT|http.+?\"\n",
    "    #pattern = \"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^RT|http.+?\"\n",
    "    for i in range(len(description)):\n",
    "        description_sub = re.sub(pattern, '',description[i])  #parse out symbols\n",
    "        text_sub        = re.sub(pattern,  '',text[i])\n",
    "        description_sub = description_sub.lower()             #lowercase\n",
    "        text_sub  = text_sub.lower()\n",
    "        description_words = description_sub.split()           #tokenize sentence\n",
    "        text_words = text_sub.split()\n",
    "\n",
    "        lemma_Dwords = []\n",
    "        lemma_Twords = []\n",
    "        stem_Dwords = []\n",
    "        stem_Twords = []\n",
    "\n",
    "        # lemmatize & stem each word in description\n",
    "        for word1 in description_words:\n",
    "            dword = lemma.lemmatize(word1)\n",
    "            dword2 = Pstem.stem(word1)\n",
    "\n",
    "            if(dword not in stopwordsSet):\n",
    "                lemma_Dwords.append(dword)\n",
    "\n",
    "            if(dword2 not in stopwordsSet):\n",
    "                stem_Dwords.append(dword2) \n",
    "        Lemma_description = ' '.join(lemma_Dwords)    \n",
    "        Stem_description = ' '.join(stem_Dwords)  \n",
    "     # lemmatize & stem each word in text\n",
    "        for word2 in text_words:\n",
    "            tword = lemma.lemmatize(word2)\n",
    "            tword2 = Pstem.stem(word2)\n",
    "            if(tword not in stopwordsSet):\n",
    "                lemma_Twords.append(tword)\n",
    "            if(tword2 not in stopwordsSet):\n",
    "                stem_Twords.append(tword2)\n",
    "        Lemma_text =  ' '.join(lemma_Twords)\n",
    "        Stem_text = ' '.join(stem_Twords)\n",
    "\n",
    "        Lemma_descriptionList.append(Lemma_description)   \n",
    "        Lemma_textList.append(Lemma_text)\n",
    "        Stem_descriptionList.append(Stem_description)\n",
    "        Stem_textList.append(Stem_text)\n",
    "        #generate set of male and female words\n",
    "        Lwordset = set(lemma_Dwords).union(set(lemma_Twords))\n",
    "        Swordset = set(stem_Dwords).union(set(stem_Twords))\n",
    "        if(gender[i]  == 'female'):\n",
    "            Lemma_femaleWords = Lemma_femaleWords.union(Lwordset)\n",
    "            Stem_femaleWords = Stem_femaleWords.union(Swordset)\n",
    "        else:\n",
    "            Lemma_maleWords = Lemma_maleWords.union(Lwordset)\n",
    "            Stem_maleWords = Stem_maleWords.union(Swordset)\n",
    "    #end of for loop for cleaning text based attributes \n",
    "    lemma_desc = pd.DataFrame(Lemma_descriptionList,columns= ['Lemma_description'])\n",
    "    lemma_text = pd.DataFrame(Lemma_textList,columns = ['Lemma_text'])\n",
    "    stem_desc = pd.DataFrame(Stem_descriptionList,columns= ['stem_description'])\n",
    "    stem_text = pd.DataFrame(Stem_textList,columns = ['stem_text'])\n",
    "    init_data = init_data.join(lemma_desc)\n",
    "    init_data = init_data.join(lemma_text)\n",
    "    init_data = init_data.join(stem_desc)\n",
    "    init_data = init_data.join(stem_text)\n",
    "    init_data = init_data.drop(['description','text'],axis=1)\n",
    "\n",
    "    if(checkifTestSet == False):\n",
    "        print(\"Generating word lists\")\n",
    "        lemma_intersect = Lemma_femaleWords.intersection(Lemma_maleWords)\n",
    "        Lemma_femaleWords = Lemma_femaleWords - lemma_intersect\n",
    "        Lemma_maleWords = Lemma_maleWords - lemma_intersect\n",
    "\n",
    "        stem_intersect = Stem_femaleWords.intersection(Stem_maleWords)\n",
    "        Stem_femaleWords = Stem_femaleWords - stem_intersect\n",
    "        Stem_maleWords = Stem_maleWords - stem_intersect\n",
    "        with open('LemmaFemaleWordList.json','w') as fp:\n",
    "            json.dump(list(Lemma_femaleWords), fp, indent=4)   \n",
    "\n",
    "        with open('LemmaMaleWordList.json','w') as fp2:\n",
    "            json.dump(list(Lemma_maleWords), fp2, indent=4)   \n",
    "\n",
    "        with open('StemFemaleWordList.json','w') as fp3:\n",
    "            json.dump(list(Stem_femaleWords), fp3, indent=4)   \n",
    "\n",
    "        with open('StemMaleWordList.json','w') as fp4:\n",
    "            json.dump(list(Stem_maleWords), fp4, indent=4)\n",
    "\n",
    "        with open('LemmaIntersectionWord.json','w') as fp5:\n",
    "            json.dump(list(lemma_intersect), fp5, indent=4)\n",
    "\n",
    "        with open('StemIntersectionWord.json','w') as fp6:\n",
    "            json.dump(list(stem_intersect), fp6, indent=4)  \n",
    "    return init_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateGenderTextCount(init_data):\n",
    "# text processing cont.  -generate the 6 new attributes - 2 for intersection of male &\n",
    "# female words , 2 for female words, 2 for male words based on lemmatization & stemming \n",
    "    lemmaMaleCount = []\n",
    "    lemmeFemaleCount = []\n",
    "    lemmaIntersectCount = []\n",
    "    stemMaleCount = []\n",
    "    stemFemaleCount = []\n",
    "    stemIntersectCount = []\n",
    "\n",
    "    lemDescription = init_data['Lemma_description']\n",
    "    lemText = init_data['Lemma_text']\n",
    "    sDescription = init_data['stem_description']\n",
    "    sText = init_data['stem_text']\n",
    "\n",
    "    for x in range(len(lemDescription)):\n",
    "        Ldescription_words = lemDescription[x].split()           #tokenize sentence\n",
    "        Ltext_words = lemText[x].split()\n",
    "        text_data = Ldescription_words+Ltext_words\n",
    "        LmaleCount = 0\n",
    "        LfemaleCount = 0\n",
    "        Lintersect = 0\n",
    "        for word in text_data:\n",
    "            if word in lemma_intersect:\n",
    "                Lintersect+=1\n",
    "            elif word in Lemma_femaleWords:\n",
    "                LfemaleCount+=1\n",
    "            elif word in Lemma_maleWords:\n",
    "                LmaleCount+=1\n",
    "        lemmaMaleCount.append(LmaleCount)\n",
    "        lemmeFemaleCount.append(LfemaleCount)\n",
    "        lemmaIntersectCount.append(Lintersect)\n",
    "\n",
    "\n",
    "    for m in range(len(sDescription)):\n",
    "        Sdescription_words = sDescription[m].split() \n",
    "        Stext_words = sText[m].split()\n",
    "        stext_data = Sdescription_words+Stext_words\n",
    "        SmaleCount = 0\n",
    "        SfemaleCount = 0\n",
    "        Sintersect = 0\n",
    "        for word in stext_data:\n",
    "            if word in stem_intersect:\n",
    "                Sintersect+=1\n",
    "            elif word in Stem_femaleWords:\n",
    "                SfemaleCount+=1\n",
    "            elif word in Stem_maleWords:\n",
    "                SmaleCount+=1\n",
    "        stemMaleCount.append(SmaleCount)\n",
    "        stemFemaleCount.append(SfemaleCount)\n",
    "        stemIntersectCount.append(Sintersect)\n",
    "    lemma_male = pd.DataFrame(lemmaMaleCount,columns= ['LMalecount'])\n",
    "    lemma_female = pd.DataFrame(lemmeFemaleCount,columns = ['LFemalecount'])\n",
    "    lemma_inter = pd.DataFrame(lemmaIntersectCount,columns= ['Lintersectcount'])\n",
    "    init_data = init_data.join(lemma_male)\n",
    "    init_data = init_data.join(lemma_female)\n",
    "    init_data = init_data.join(lemma_inter)\n",
    "\n",
    "    stem_male = pd.DataFrame(stemMaleCount,columns= ['SMalecount'])\n",
    "    stem_female = pd.DataFrame(stemFemaleCount,columns= ['SFemalecount'])\n",
    "    stem_inter = pd.DataFrame(stemIntersectCount,columns= ['Sintersectcount'])\n",
    "    init_data = init_data.join(stem_male)\n",
    "    init_data = init_data.join(stem_female)\n",
    "    init_data = init_data.join(stem_inter)\n",
    "    return init_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nameProcessing(init_data):\n",
    "# name pre-processing - Generate 2 new attributes for number of vowels and constants from name\n",
    "#last letter of female & male names obtained from https://home.uchicago.edu/~jsfalk/misc/baby_names/\n",
    "    names = init_data['name']\n",
    "    pattern = \"(@[A-Za-z]+)|([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^RT|http.+?\"\n",
    "    nameCount = []\n",
    "    for name in names:   \n",
    "        clean_name = (re.sub(pattern, '',name)).lower()\n",
    "        clean_name = clean_name.replace(\" \", \"\")\n",
    "        vowel=len([letter for letter in clean_name if letter in \"aeiou\"])\n",
    "        constant=len([letter for letter in clean_name if letter not in \"aeiou\"])\n",
    "        nameCount.append((vowel,constant))\n",
    "    CountofNames = pd.DataFrame(nameCount,columns= ['vowel','constant'])\n",
    "    init_data = init_data.join(CountofNames)\n",
    "    return init_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replacements(init_data):\n",
    "    # For numeric attributes - apply log function to reduce range and prevent data skew\n",
    "    # replace all 0's with 1's to prevent div by 0 error - still get 0.\n",
    "    init_data.fav_number.replace(0,1, inplace=True)\n",
    "    init_data.tweet_count.replace(0,1, inplace=True)\n",
    "    init_data['fav_number'] = np.log(init_data.fav_number)\n",
    "    init_data['tweet_count'] = np.log(init_data.tweet_count)\n",
    "    return init_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word lists\n"
     ]
    }
   ],
   "source": [
    "# Load & clean in training data for cleaning & generating word sets\n",
    "training_data = pd.read_csv(\"gender-classifier-DFE-791531.csv\",engine=\"python\");\n",
    "column_names = training_data.iloc[0]\n",
    "training_data.rename(index=str, columns=column_names)\n",
    "lemma_intersect = set()\n",
    "Lemma_femaleWords =set()\n",
    "Lemma_maleWords =set()\n",
    "stem_intersect =set()\n",
    "Stem_femaleWords =set()\n",
    "Stem_maleWords = set()\n",
    "training_data = featureSelect(training_data)\n",
    "training_data = colorReplacement(training_data)\n",
    "training_data = textProcess(training_data,False)\n",
    "training_data = generateGenderTextCount(training_data)\n",
    "training_data = nameProcessing(training_data)\n",
    "training_data = replacements(training_data)\n",
    "\n",
    "#Write back all new data to csv\n",
    "training_data.to_csv('clean_data.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load & clean real dataset\n",
    "# Mofidy this line to file name of file being tested\n",
    "test_set = pd.read_csv('unclean_sample.csv',engine=\"python\")\n",
    "column_names = test_set.iloc[0]\n",
    "test_set.rename(index=str, columns=column_names)\n",
    "test_set = featureSelect(test_set)\n",
    "test_set = colorReplacement(test_set)\n",
    "test_set = textProcess(test_set,True)\n",
    "test_set = generateGenderTextCount(test_set)\n",
    "test_set = nameProcessing(test_set)\n",
    "test_set = replacements(test_set)\n",
    "test_set.to_csv('realdata.csv', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fav_number          6.637782\n",
      "tweet_count         8.881898\n",
      "sred                0.620678\n",
      "sgreen              0.666835\n",
      "sblue               0.689925\n",
      "lred                0.314044\n",
      "lgreen              0.426762\n",
      "lblue               0.566201\n",
      "LMalecount          0.000000\n",
      "LFemalecount        2.172687\n",
      "Lintersectcount     9.214030\n",
      "SMalecount          0.000000\n",
      "SFemalecount        1.914328\n",
      "Sintersectcount    10.102090\n",
      "vowel               4.064030\n",
      "constant            6.528060\n",
      "dtype: float64\n",
      "fav_number          6.214485\n",
      "tweet_count         8.837328\n",
      "sred                0.625360\n",
      "sgreen              0.687410\n",
      "sblue               0.711276\n",
      "lred                0.192417\n",
      "lgreen              0.440350\n",
      "lblue               0.568305\n",
      "LMalecount          2.718760\n",
      "LFemalecount        0.000000\n",
      "Lintersectcount    10.312561\n",
      "SMalecount          2.371650\n",
      "SFemalecount        0.000000\n",
      "Sintersectcount    11.278495\n",
      "vowel               3.551179\n",
      "constant            6.501614\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "female_data = training_data[(training_data.gender == 'female')]\n",
    "fmean = female_data.mean()\n",
    "male_data = training_data[(training_data.gender == 'male')]\n",
    "mmean = male_data.mean()\n",
    "print(fmean)\n",
    "print(mmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate sample unclean file to test functions\n",
    "# unclean = pd.read_csv(\"gender-classifier-DFE-791531.csv\",engine=\"python\");\n",
    "# unclean2 = unclean.head(10000)\n",
    "# unclean.to_csv('unclean_sample.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Everything below is for beginning analysis purposes\n",
    "# training_data = pd.read_csv(\"gender-classifier-DFE-791531.csv\",engine=\"python\");\n",
    "# column_names = training_data.iloc[0]\n",
    "# training_data.rename(index=str, columns=column_names)\n",
    "# #check na values\n",
    "# training_data.isnull().sum()\n",
    "# drop_attributes = ['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
    "#                    '_last_judgment_at','gender:confidence', 'profile_yn',\n",
    "#                    'profile_yn:confidence', 'created','gender_gold','profile_yn_gold',\n",
    "#                    'tweet_created', 'tweet_id']\n",
    "# training_data = training_data.drop(drop_attributes,axis=1)\n",
    "# #Remove attributes with too many miss values - tweet_coord\n",
    "# training_data = training_data.drop(['tweet_coord'],axis=1)\n",
    "# #Remove rows with missing gender, unknown gender , or is brand gender - lose around 1/3 of the data\n",
    "# training_data = training_data.dropna(subset=['gender'])\n",
    "# training_data = training_data[training_data.gender != 'brand']\n",
    "# training_data = training_data[training_data.gender != 'unknown']\n",
    "\n",
    "# # #drop retweet count because only 2.6% of male & female accounts combined have value > 0\n",
    "# # training_data = training_data.drop(['retweet_count'],axis=1)\n",
    "# training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(training_data['fav_number'].max())\n",
    "# print(training_data['tweet_count'].max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
