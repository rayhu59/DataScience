{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # for project testing - read in csv, drop some attributes, shuffle data and form training & testing set    \n",
    "testfile = \"realdata.csv\"\n",
    "#read In Training data\n",
    "data = pd.read_csv(\"clean_data.csv\");\n",
    "data.gender.replace('male',0, inplace=True)\n",
    "data.gender.replace('female',1, inplace=True)\n",
    "trainLabels = data['gender'].values            #stores all the labels\n",
    "#utilize all data\n",
    "drop_attributes = ['gender','name','Lemma_description', 'Lemma_text','stem_description', 'stem_text']\n",
    "#utilize everything but text data\n",
    "#drop_attributes = ['gender','name','Lemma_description', 'Lemma_text','stem_description', 'stem_text',\n",
    "#                   'LMalecount', 'LFemalecount', 'Lintersectcount','SMalecount', 'SFemalecount', 'Sintersectcount']\n",
    "traindata = data.drop(drop_attributes,axis=1)   # store all the data\n",
    "\n",
    "#Read In Test Data\n",
    "test_data = pd.read_csv(testfile)\n",
    " #set up data for classifying\n",
    "test_data.gender.replace('male',0, inplace=True)\n",
    "test_data.gender.replace('female',1, inplace=True)\n",
    "shuffle = test_data.sample(frac=1).reset_index(drop=True) #shuffle data twice\n",
    "test_data = shuffle.sample(frac=1).reset_index(drop=True)\n",
    "testLabels = test_data['gender'].values                   #stores test labels\n",
    "drop_attributes = ['gender','name','Lemma_description', 'Lemma_text','stem_description', 'stem_text']\n",
    "#drop_attributes = ['gender','name','Lemma_description', 'Lemma_text','stem_description', 'stem_text',\n",
    "#                   'LMalecount', 'LFemalecount', 'Lintersectcount','SMalecount', 'SFemalecount', 'Sintersectcount']\n",
    "testdata = test_data.drop(drop_attributes,axis=1)        #stores test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#knn & bagging project validation -uses text data -\n",
    "def knnTest():\n",
    "    knn = KNeighborsClassifier(n_neighbors=9)\n",
    "    knn.fit(traindata.values,trainLabels)\n",
    "    knn_score = np.mean(cross_val_score(knn,testdata.values,testLabels ,cv=10))\n",
    "    print(\"knn score : \" + str(knn_score) + \" with neighbors = \" + str(9))\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=9)\n",
    "    knn_bag = BaggingClassifier(knn,max_samples=0.5, max_features=0.5)\n",
    "    knn_bag.fit(traindata.values,trainLabels)\n",
    "    knn_bag_score = np.mean(cross_val_score(knn_bag,testdata.values,testLabels ,cv=10))\n",
    "    print(\"knn bag score : \" + str(knn_bag_score) + \" with neighbors = \" + str(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random forest, adaboost with random forest, adaboost with decisiontree\n",
    "def randForest():\n",
    "        randf = RandomForestClassifier(min_samples_leaf = 300)\n",
    "        randf.fit(traindata.values,trainLabels)\n",
    "        randf_score = np.mean(cross_val_score(randf,testdata.values,testLabels ,cv=10))\n",
    "        print(\"RandForest Gini score : \" + str(randf_score))\n",
    "        \n",
    "        randE = RandomForestClassifier(criterion  = 'entropy',min_samples_leaf = 300)\n",
    "        randE.fit(traindata.values,trainLabels)\n",
    "        randE_score = np.mean(cross_val_score(randE,testdata.values,testLabels ,cv=10))\n",
    "        print(\"RandForest Entropy score : \" + str(randE_score))\n",
    "        \n",
    "        randf = RandomForestClassifier(min_samples_leaf = 300)\n",
    "        ada = AdaBoostClassifier(base_estimator = randf)\n",
    "        ada.fit(traindata.values,trainLabels)\n",
    "        ada_score = np.mean(cross_val_score(ada,testdata.values,testLabels ,cv=10))\n",
    "        print(\"Adaboost with RandForest Gini score : \" + str(ada_score))\n",
    "        \n",
    "        randE = RandomForestClassifier(criterion  = 'entropy',min_samples_leaf = 300)\n",
    "        adaE = AdaBoostClassifier(base_estimator = randE)\n",
    "        adaE.fit(traindata.values,trainLabels)\n",
    "        adaE_score = np.mean(cross_val_score(adaE,testdata.values,testLabels ,cv=10))\n",
    "        print(\"Adaboost with RandForest Entropy score : \" + str(adaE_score))\n",
    "        \n",
    "        ada_decision = AdaBoostClassifier()\n",
    "        ada_decision.fit(traindata.values,trainLabels)\n",
    "        ada_dec_score = np.mean(cross_val_score(ada_decision,testdata.values,testLabels ,cv=10))\n",
    "        print(\"Adaboost with Decision Tree score : \" + str(ada_dec_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GaussianNB,MultinomialNB,MultinomialNB with adaboost\n",
    "def bayes():\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(traindata.values,trainLabels)\n",
    "    gnb_score = np.mean(cross_val_score(gnb,testdata.values,testLabels ,cv=10))\n",
    "    print(\"GaussianNaiveBayes score: \" + str(gnb_score))\n",
    "    \n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(traindata.values,trainLabels)\n",
    "    mnb_score = np.mean(cross_val_score(mnb,testdata.values,testLabels ,cv=10))\n",
    "    print(\"MultinomialNaiveBayes score: \" + str(mnb_score))\n",
    "    \n",
    "    mnb = MultinomialNB()\n",
    "    ada = AdaBoostClassifier(base_estimator = mnb)\n",
    "    ada.fit(traindata.values,trainLabels)\n",
    "    ada_score = np.mean(cross_val_score(ada,testdata.values,testLabels ,cv=10)) \n",
    "    print(\"Adaboost with MultinomialNaiveBayes score : \" + str(ada_score))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SVC\n",
    "def svmTest():\n",
    "    svc = SVC(gamma='auto')\n",
    "    svc.fit(traindata.values,trainLabels)\n",
    "    svc_score = np.mean(cross_val_score(svc,testdata.values,testLabels ,cv=10))\n",
    "    print(\"svc score: \" + str(svc_score))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for analysis purposes\n",
    "def knnAnalysis():\n",
    "    for testSize in range(1,10):\n",
    "        for n in range(1,10):\n",
    "            train_set, test_set, train_labels, test_label= train_test_split( analysis_data.values, analysis_label, test_size=testSize/10)   \n",
    "            knn = KNeighborsClassifier(n_neighbors=n)\n",
    "            knn.fit(train_set,train_labels)\n",
    "            curr_score = np.mean(cross_val_score(knn,test_set,test_label ,cv=10))\n",
    "            #uncomment to manually find out of 100-the best size\n",
    "            print(\"knn score : \" + str(curr_score)  + \" for testsize of \" + str(testSize/10) + \" with neighbors = \" + str(n))\n",
    "             \n",
    "    #bag using optimal size from above, 9 neighbors is the best        \n",
    "    train_set, test_set, train_labels, test_label= train_test_split( analysis_data.values, analysis_label, test_size=0.6)   \n",
    "    for i in range(1,10):\n",
    "        knn = KNeighborsClassifier(n_neighbors=i)\n",
    "        knn_bag = BaggingClassifier(knn,max_samples=0.5, max_features=0.5)\n",
    "        knn_bag.fit(train_set,train_labels)\n",
    "        prediction = knn_bag.predict(test_set)\n",
    "        print(\"knn bag score : \" + str(accuracy_score(prediction,test_label)) + \" for neighbors = \" + str(i))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randForestAnalysis():\n",
    "    train_set, test_set, train_labels, test_label= train_test_split( analysis_data.values, analysis_label, test_size=0.6)   \n",
    "    randGini = RandomForestClassifier()\n",
    "    randEntropy = RandomForestClassifier(criterion  = 'entropy')\n",
    "    randGini.fit(train_set,train_labels)\n",
    "    randEntropy.fit(train_set,train_labels)\n",
    "    randGini_score = np.mean(cross_val_score(randGini,test_set,test_label ,cv=10))\n",
    "    randEntropy_score = np.mean(cross_val_score(randEntropy,test_set,test_label ,cv=10))\n",
    "    print(\"RandomForest Gini score : \" + str(randGini_score))\n",
    "    print(\"RandomForest Entropy score: \" + str(randEntropy_score))\n",
    "    \n",
    "    for n in range(1,11):\n",
    "        randGini = RandomForestClassifier(min_samples_leaf = n*100)\n",
    "        randEntropy = RandomForestClassifier(criterion  = 'entropy',min_samples_leaf = n*100) \n",
    "        randGini.fit(train_set,train_labels)\n",
    "        randEntropy.fit(train_set,train_labels)\n",
    "        randGini_score = np.mean(cross_val_score(randGini,test_set,test_label ,cv=10))\n",
    "        randEntropy_score = np.mean(cross_val_score(randEntropy,test_set,test_label ,cv=10))\n",
    "        print(\"RandomForest Gini score with min leaf \" + str(n*100) + \" = \" + str(randGini_score))\n",
    "        print(\"RandomForest Entropy score with min leaf \"+ str(n*100) + \" = \" + str(randEntropy_score))\n",
    "\n",
    "    randf = RandomForestClassifier(min_samples_leaf = 300)\n",
    "    ada = AdaBoostClassifier(base_estimator = randf)\n",
    "    ada.fit(train_set,train_labels)\n",
    "    ada_score = np.mean(cross_val_score(ada,test_set,test_label ,cv=10))\n",
    "    print(\"Adaboost with RandForest Gini score : \" + str(ada_score))\n",
    "        \n",
    "    randE = RandomForestClassifier(criterion  = 'entropy',min_samples_leaf = 300)\n",
    "    adaE = AdaBoostClassifier(base_estimator = randE)\n",
    "    adaE.fit(train_set,train_labels)\n",
    "    adaE_score = np.mean(cross_val_score(adaE,test_set,test_label,cv=10))\n",
    "    print(\"Adaboost with RandForest Entropy score : \" + str(adaE_score))\n",
    "\n",
    "    ada_decision = AdaBoostClassifier()\n",
    "    ada_decision.fit(train_set,train_labels)\n",
    "    ada_dec_score = np.mean(cross_val_score(ada_decision,test_set,test_label,cv=10))\n",
    "    print(\"Adaboost with Decision Tree score : \" + str(ada_dec_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bayesAnalysis():\n",
    "    train_set, test_set, train_labels, test_label= train_test_split( analysis_data.values, analysis_label, test_size=0.6) \n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(train_set,train_labels)\n",
    "    gnb_score = np.mean(cross_val_score(gnb,test_set,test_label ,cv=10))\n",
    "    print(\"GaussianNB score : \" + str(gnb_score))\n",
    "    \n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(train_set,train_labels)\n",
    "    mnb_score = np.mean(cross_val_score(mnb,test_set,test_label ,cv=10))\n",
    "    print(\"MultinomialNB score : \" + str(mnb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svcAnalysis():\n",
    "    train_set, test_set, train_labels, test_label= train_test_split( analysis_data.values, analysis_label, test_size=0.6) \n",
    "    svc = SVC(gamma='auto')\n",
    "    svc.fit(train_set,train_labels)\n",
    "    svc_score = np.mean(cross_val_score(svc,test_set,test_label ,cv=10))\n",
    "    print(\"SVC : \" + str(svc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn score : 0.903752803988 with neighbors = 9\n",
      "knn bag score : 0.912129407449 with neighbors = 9\n",
      "RandForest Gini score : 0.923994443141\n",
      "RandForest Entropy score : 0.923140888015\n",
      "Adaboost with RandForest Gini score : 0.920272310126\n",
      "Adaboost with RandForest Entropy score : 0.921668681329\n",
      "Adaboost with Decision Tree score : 0.924227542533\n",
      "GaussianNaiveBayes score: 0.92570119256\n",
      "MultinomialNaiveBayes score: 0.923063669331\n",
      "Adaboost with MultinomialNaiveBayes score : 0.618040485684\n",
      "svc score: 0.924148880509\n"
     ]
    }
   ],
   "source": [
    "#For project grade -just take the highest accuracy among the classifers\n",
    "knnTest()\n",
    "randForest()\n",
    "bayes()\n",
    "svmTest()\n",
    "\n",
    "#everything below is for analysis purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for running analysis\n",
    "#ctrl / for uncomment\n",
    "# analysis_data = pd.read_csv(\"clean_data.csv\");\n",
    "# analysis_data.gender.replace('male',0, inplace=True)\n",
    "# analysis_data.gender.replace('female',1, inplace=True)\n",
    "# analysis_label = analysis_data['gender'].values\n",
    "# #utilize all data\n",
    "# drop_attributes = ['gender','name','Lemma_description', 'Lemma_text','stem_description', 'stem_text']\n",
    "\n",
    "# #use lemmatization of text only\n",
    "# #drop_attributes = ['gender','name','Lemma_description', 'Lemma_text','stem_description', 'stem_text',\n",
    "# #                   'LMalecount', 'LFemalecount', 'Lintersectcount']\n",
    "\n",
    "# #use stemming of text only\n",
    "# # drop_attributes = ['gender','name','Lemma_description', 'Lemma_text','stem_description', 'stem_text',\n",
    "# #                    'SMalecount', 'SFemalecount', 'Sintersectcount']\n",
    "\n",
    "# #don't use text data\n",
    "# #drop_attributes = ['gender','name','Lemma_description', 'Lemma_text','stem_description', 'stem_text',\n",
    "# #                    'LMalecount', 'LFemalecount', 'Lintersectcount','SMalecount', 'SFemalecount', 'Sintersectcount']\n",
    "\n",
    "# #use text data only\n",
    "# # drop_attributes = ['gender','name','Lemma_description', 'Lemma_text','stem_description', 'stem_text',\n",
    "# #                    'fav_number','tweet_count', 'sred', 'sgreen','sblue', 'lred', 'lgreen', 'lblue']\n",
    "\n",
    "\n",
    "# analysis_data = analysis_data.drop(drop_attributes,axis=1)\n",
    "# knnAnalysis()\n",
    "# randForestAnalysis()\n",
    "# bayesAnalysis()\n",
    "# svcAnalysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
